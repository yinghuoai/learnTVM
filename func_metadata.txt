func_metadata: {"tvmgen_default_fused_ones": FunctionInfoNode(
workspace_sizes={llvm -keys=cpu -link-params=0: 0},
  io_sizes={llvm -keys=cpu -link-params=0: 4},
  constant_sizes={llvm -keys=cpu -link-params=0: 0},
  tir_primfuncs={llvm -keys=cpu -link-params=0: PrimFunc([T_full]) attrs={"from_legacy_te_schedule": (bool)1, "global_symbol": "tvmgen_default_fused_ones", "tir.noalias": (bool)1} {
  T_full[0] = 1f
}
},
  relay_primfuncs={llvm -keys=cpu -link-params=0: fn (hash="8559812ceca04b27", prim_funcs={'tvmgen_default_fused_ones'=meta[tir.PrimFunc][0]}, target=meta[Target][0], Primitive=1, prim_fn_var='tvmgen_default_fused_ones') -> Tensor[(1, 1, 1, 1), float32] {
  ones(shape=[1, 1, 1, 1], dtype="float32") /* ty=Tensor[(1, 1, 1, 1), float32] */
}
}), "tvmgen_default_fused_nn_conv2d_power_divide": FunctionInfoNode(
workspace_sizes={llvm -keys=cpu -link-params=0: 112},
  io_sizes={llvm -keys=cpu -link-params=0: 36},
  constant_sizes={llvm -keys=cpu -link-params=0: 0},
  tir_primfuncs={llvm -keys=cpu -link-params=0: PrimFunc([placeholder, placeholder, T_divide]) attrs={"from_legacy_te_schedule": (bool)1, "global_symbol": "tvmgen_default_fused_nn_conv2d_power_divide", "tir.noalias": (bool)1} {
  allocate data_vec[float32 * 9], storage_scope = global
  allocate kernel_vec[float32 * 1], storage_scope = global
  allocate conv2d_NCHWc.global[float32 * 9], storage_scope = global
  parallel (bs.c.fused.h.fused, 0, 3) {
    for (w, 0, 3) {
      let cse_var_1 = ((bs.c.fused.h.fused*3) + w)
      data_vec[cse_var_1] = placeholder[cse_var_1]
    }
  }
  kernel_vec[0] = placeholder[0]
  conv2d_NCHWc.global[0] = 0f
  conv2d_NCHWc.global[1] = 0f
  conv2d_NCHWc.global[2] = 0f
  conv2d_NCHWc.global[3] = 0f
  conv2d_NCHWc.global[4] = 0f
  conv2d_NCHWc.global[5] = 0f
  conv2d_NCHWc.global[6] = 0f
  conv2d_NCHWc.global[7] = 0f
  conv2d_NCHWc.global[8] = 0f
  conv2d_NCHWc.global[0] = (conv2d_NCHWc.global[0] + (data_vec[0]*kernel_vec[0]))
  conv2d_NCHWc.global[1] = (conv2d_NCHWc.global[1] + (data_vec[1]*kernel_vec[0]))
  conv2d_NCHWc.global[2] = (conv2d_NCHWc.global[2] + (data_vec[2]*kernel_vec[0]))
  conv2d_NCHWc.global[3] = (conv2d_NCHWc.global[3] + (data_vec[3]*kernel_vec[0]))
  conv2d_NCHWc.global[4] = (conv2d_NCHWc.global[4] + (data_vec[4]*kernel_vec[0]))
  conv2d_NCHWc.global[5] = (conv2d_NCHWc.global[5] + (data_vec[5]*kernel_vec[0]))
  conv2d_NCHWc.global[6] = (conv2d_NCHWc.global[6] + (data_vec[6]*kernel_vec[0]))
  conv2d_NCHWc.global[7] = (conv2d_NCHWc.global[7] + (data_vec[7]*kernel_vec[0]))
  conv2d_NCHWc.global[8] = (conv2d_NCHWc.global[8] + (data_vec[8]*kernel_vec[0]))
  for (ax2.inner, 0, 3) {
    for (ax3.inner, 0, 3) {
      let cse_var_2 = ((ax2.inner*3) + ax3.inner)
      T_divide[cse_var_2] = (tir.pow(conv2d_NCHWc.global[cse_var_2], -2f)*0.5f)
    }
  }
}
},
  relay_primfuncs={llvm -keys=cpu -link-params=0: fn (%p0: Tensor[(1, 1, 3, 3), float32], %p1: Tensor[(1, 1, 1, 1), float32], target=meta[Target][0], prim_funcs={'tvmgen_default_fused_nn_conv2d_power_divide'=meta[tir.PrimFunc][0]}, out_layout="", data_layout="NCHW", hash="1291013955fd99f5", kernel_layout="OIHW", prim_fn_var='tvmgen_default_fused_nn_conv2d_power_divide', Primitive=1) -> Tensor[(1, 1, 3, 3), float32] {
  %0 = nn.conv2d(%p0, %p1, padding=[0, 0, 0, 0], channels=1, kernel_size=[1, 1]) /* ty=Tensor[(1, 1, 3, 3), float32] */;
  %1 = power(%0, -2f /* ty=float32 */) /* ty=Tensor[(1, 1, 3, 3), float32] */;
  divide(%1, 2f /* ty=float32 */) /* ty=Tensor[(1, 1, 3, 3), float32] */
}
}), "__tvm_main__": FunctionInfoNode(
workspace_sizes={llvm -keys=cpu -link-params=0: 4},
  io_sizes={llvm -keys=cpu -link-params=0: 72},
  constant_sizes={llvm -keys=cpu -link-params=0: 0},
  tir_primfuncs={},
  relay_primfuncs={llvm -keys=cpu -link-params=0: fn (%input1: Tensor[(1, 1, 3, 3), float32], param_virtual_devices=[VirtualDevice(device_type=1, virtual_device_id=0, target=Target(kind='llvm', keys={'cpu'}, attrs={'link-params': (bool)0}, host=Target(kind='llvm', keys={'cpu'}, attrs={'link-params': (bool)0})))], hash="f7298b0e507e0d80", result_virtual_device=VirtualDevice(device_type=1, virtual_device_id=0, target=Target(kind='llvm', keys={'cpu'}, attrs={'link-params': (bool)0}, host=Target(kind='llvm', keys={'cpu'}, attrs={'link-params': (bool)0})))) -> Tensor[(1, 1, 3, 3), float32] {
  %2 = fn (Primitive=1, hash="8559812ceca04b27") -> Tensor[(1, 1, 1, 1), float32] {
    ones(shape=[1, 1, 1, 1], dtype="float32") /* ty=Tensor[(1, 1, 1, 1), float32] */
  };
  %3 = %2() /* ty=Tensor[(1, 1, 1, 1), float32] */;
  %4 = fn (%p0: Tensor[(1, 1, 3, 3), float32], %p1: Tensor[(1, 1, 1, 1), float32], hash="1291013955fd99f5", data_layout="NCHW", kernel_layout="OIHW", Primitive=1, out_layout="") -> Tensor[(1, 1, 3, 3), float32] {
    %0 = nn.conv2d(%p0, %p1, padding=[0, 0, 0, 0], channels=1, kernel_size=[1, 1]) /* ty=Tensor[(1, 1, 3, 3), float32] */;
    %1 = power(%0, -2f /* ty=float32 */) /* ty=Tensor[(1, 1, 3, 3), float32] */;
    divide(%1, 2f /* ty=float32 */) /* ty=Tensor[(1, 1, 3, 3), float32] */
  };
  %4(%input1, %3) /* ty=Tensor[(1, 1, 3, 3), float32] */
}})}
